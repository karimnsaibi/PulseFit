{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PulseFit ETL Process: From Raw to Clean Data\n",
    "**Objective**: Extract data from raw CSV files, perform cleaning and transformations, and load the results into a processed format suitable for BI analysis.\n",
    "\n",
    "## Phase 1: Extraction & Initial Inspection\n",
    "In this phase, we load our datasets and perform a high-level scan to understand the quality of the data we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Paths\n",
    "RAW_DATA_PATH = \"data/raw/\"\n",
    "PROCESSED_DATA_PATH = \"data/processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Members: 500 rows\n",
      "- Visits: 1996 rows\n",
      "- Payments: 1500 rows\n"
     ]
    }
   ],
   "source": [
    "members_raw = pd.read_csv(os.path.join(RAW_DATA_PATH, \"members_raw.csv\"))\n",
    "visits_raw = pd.read_csv(os.path.join(RAW_DATA_PATH, \"visits_raw.csv\"))\n",
    "payments_raw = pd.read_csv(os.path.join(RAW_DATA_PATH, \"payments_raw.csv\"))\n",
    "\n",
    "print(f\"- Members: {members_raw.shape[0]} rows\")\n",
    "print(f\"- Visits: {visits_raw.shape[0]} rows\")\n",
    "print(f\"- Payments: {payments_raw.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initial Data Quality Audit\n",
    "Let's look at the structure and find basic issues like missing values and wrong data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Members Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   member_id        500 non-null    object\n",
      " 1   full_name        500 non-null    object\n",
      " 2   email            476 non-null    object\n",
      " 3   phone_number     500 non-null    object\n",
      " 4   gender           494 non-null    object\n",
      " 5   join_date        500 non-null    object\n",
      " 6   membership_type  500 non-null    object\n",
      " 7   status           500 non-null    object\n",
      " 8   home_branch      500 non-null    object\n",
      " 9   birth_year       500 non-null    int64 \n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 39.2+ KB\n",
      "None\n",
      "\n",
      "--- Visits Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1996 entries, 0 to 1995\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   visit_id        1996 non-null   object\n",
      " 1   member_id       1996 non-null   object\n",
      " 2   branch          1996 non-null   object\n",
      " 3   check_in_time   1996 non-null   object\n",
      " 4   check_out_time  1798 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 78.1+ KB\n",
      "None\n",
      "\n",
      "--- Payments Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   payment_id       1500 non-null   object\n",
      " 1   member_id        1500 non-null   object\n",
      " 2   date             1500 non-null   object\n",
      " 3   amount           1500 non-null   int64 \n",
      " 4   payment_method   1500 non-null   object\n",
      " 5   membership_type  1500 non-null   object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 70.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Members Info ---\")\n",
    "print(members_raw.info())\n",
    "print(\"\\n--- Visits Info ---\")\n",
    "print(visits_raw.info())\n",
    "print(\"\\n--- Payments Info ---\")\n",
    "print(payments_raw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Identifying Inconsistencies\n",
    "Scanning for the 'human errors' like branch name typos or gender inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Branches in Members: ['Nabeul Plage' 'Sousse Corniche' 'Sfax Center' 'Nabeul' 'Bizerte Port'\n",
      " 'Sfax_Center' 'tunis lake' 'Sousse' 'Tunis Lake']\n",
      "\n",
      "Unique Genders in Members: ['M' 'F' 'Male' nan 'Femal']\n",
      "\n",
      "Membership Types: ['Annual' 'VIP' '6-Month' 'Student' 'Monthly' 'MONTHLY' 'ANNUAL' 'STUDENT'\n",
      " '6-MONTH']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Branches in Members:\", members_raw['home_branch'].unique())\n",
    "print(\"\\nUnique Genders in Members:\", members_raw['gender'].unique())\n",
    "print(\"\\nMembership Types:\", members_raw['membership_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Detailed Quality Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Missing Values Check:\n",
      "member_id           0\n",
      "full_name           0\n",
      "email              24\n",
      "phone_number        0\n",
      "gender              6\n",
      "join_date           0\n",
      "membership_type     0\n",
      "status              0\n",
      "home_branch         0\n",
      "birth_year          0\n",
      "dtype: int64\n",
      "\n",
      "   Duplicate Members Check:\n",
      "Found 307 potential duplicate records.\n",
      "\n",
      "   Value Range Checks:\n",
      "Payments Amount Min/Max: -450 / 8000\n",
      "Visits Check-out Missing: 198\n"
     ]
    }
   ],
   "source": [
    "print(\"     Missing Values Check:\")\n",
    "print(members_raw.isnull().sum())\n",
    "\n",
    "print(\"\\n   Duplicate Members Check:\")\n",
    "dups = members_raw[members_raw.duplicated(subset=['full_name', 'email'], keep=False)]\n",
    "print(f\"Found {len(dups)} potential duplicate records.\")\n",
    "\n",
    "print(\"\\n   Value Range Checks:\")\n",
    "print(\"Payments Amount Min/Max:\", payments_raw['amount'].min(), \"/\", payments_raw['amount'].max())\n",
    "print(\"Visits Check-out Missing:\", visits_raw['check_out_time'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Data Cleaning & Transformation\n",
    "\n",
    "### 2.1 Cleaning Members Data\n",
    "- Normalize Branch Names\n",
    "- Fix Gender Categories\n",
    "- Deduplicate members\n",
    "- Handle missing emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Members dataset after cleaning: 323 rows\n"
     ]
    }
   ],
   "source": [
    "members_clean = members_raw.copy()\n",
    "\n",
    "# 1. Standardize Branch names (title case and fix common typos)\n",
    "branch_map = {\n",
    "    'tunis lake': 'Tunis Lake', \n",
    "    'Sfax_Center': 'Sfax Center', \n",
    "    'Sousse': 'Sousse Corniche', # Assuming they're the same branch\n",
    "    'Nabeul': 'Nabeul Plage'\n",
    "}\n",
    "members_clean['home_branch'] = members_clean['home_branch'].replace(branch_map)\n",
    "\n",
    "# 2. Standardize Gender\n",
    "members_clean['gender'] = members_clean['gender'].replace({'Male': 'M', 'Femal': 'F'})\n",
    "members_clean['gender'] = members_clean['gender'].fillna('U') # U for Unknown\n",
    "\n",
    "# 3. Remove Duplicates (Keeping first occurrence)\n",
    "members_clean = members_clean.drop_duplicates(subset=['full_name', 'email'], keep='first')\n",
    "\n",
    "print(f\"Members dataset after cleaning: {members_clean.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cleaning Visits Data\n",
    "- Convert timestamps to datetime objects\n",
    "- Impute missing check-out times (assume 1h duration)\n",
    "- Fix logic errors (check-out < check-in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the clean version\n",
    "visits_clean = visits_raw.copy()\n",
    "# 2. Convert raw strings to proper Datetime objects\n",
    "visits_clean['check_in_time'] = pd.to_datetime(visits_clean['check_in_time'])\n",
    "visits_clean['check_out_time'] = pd.to_datetime(visits_clean['check_out_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>branch</th>\n",
       "      <th>check_in_time</th>\n",
       "      <th>check_out_time</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c7b42034</td>\n",
       "      <td>26e8e627</td>\n",
       "      <td>Tunis Lake</td>\n",
       "      <td>2025-06-08 18:02:00</td>\n",
       "      <td>2025-06-08 18:40:00</td>\n",
       "      <td>0 days 00:38:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9cb80428</td>\n",
       "      <td>b357874e</td>\n",
       "      <td>Nabeul</td>\n",
       "      <td>2023-12-14 09:27:00</td>\n",
       "      <td>2023-12-14 11:24:00</td>\n",
       "      <td>0 days 01:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718c9145</td>\n",
       "      <td>a4ed006b</td>\n",
       "      <td>Nabeul</td>\n",
       "      <td>2023-03-29 10:03:00</td>\n",
       "      <td>2023-03-29 11:16:00</td>\n",
       "      <td>0 days 01:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e0f08686</td>\n",
       "      <td>6971d7b6</td>\n",
       "      <td>Nabeul Plage</td>\n",
       "      <td>2024-09-25 13:16:00</td>\n",
       "      <td>2024-09-25 15:12:00</td>\n",
       "      <td>0 days 01:56:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7b05197e</td>\n",
       "      <td>eb517e91</td>\n",
       "      <td>Sfax_Center</td>\n",
       "      <td>2025-06-14 08:05:00</td>\n",
       "      <td>2025-06-14 09:50:00</td>\n",
       "      <td>0 days 01:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>36fee19b</td>\n",
       "      <td>fe773c65</td>\n",
       "      <td>tunis lake</td>\n",
       "      <td>2024-01-23 14:56:00</td>\n",
       "      <td>2024-01-23 15:53:00</td>\n",
       "      <td>0 days 00:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>3efa1f11</td>\n",
       "      <td>966e2a16</td>\n",
       "      <td>Sfax Center</td>\n",
       "      <td>2024-06-24 06:10:00</td>\n",
       "      <td>2024-06-24 08:09:00</td>\n",
       "      <td>0 days 01:59:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>bf5f7820</td>\n",
       "      <td>dd49cdfe</td>\n",
       "      <td>Sfax Center</td>\n",
       "      <td>2025-01-06 21:20:00</td>\n",
       "      <td>2025-01-06 23:08:00</td>\n",
       "      <td>0 days 01:48:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>eadc0e36</td>\n",
       "      <td>cfcb50a1</td>\n",
       "      <td>Sfax_Center</td>\n",
       "      <td>2024-04-08 12:49:00</td>\n",
       "      <td>2024-04-08 13:24:00</td>\n",
       "      <td>0 days 00:35:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>c35f12c9</td>\n",
       "      <td>03d42852</td>\n",
       "      <td>Tunis Lake</td>\n",
       "      <td>2024-07-02 12:01:00</td>\n",
       "      <td>2024-07-02 13:47:00</td>\n",
       "      <td>0 days 01:46:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1798 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      visit_id member_id        branch       check_in_time  \\\n",
       "0     c7b42034  26e8e627    Tunis Lake 2025-06-08 18:02:00   \n",
       "1     9cb80428  b357874e        Nabeul 2023-12-14 09:27:00   \n",
       "2     718c9145  a4ed006b        Nabeul 2023-03-29 10:03:00   \n",
       "3     e0f08686  6971d7b6  Nabeul Plage 2024-09-25 13:16:00   \n",
       "5     7b05197e  eb517e91   Sfax_Center 2025-06-14 08:05:00   \n",
       "...        ...       ...           ...                 ...   \n",
       "1990  36fee19b  fe773c65    tunis lake 2024-01-23 14:56:00   \n",
       "1991  3efa1f11  966e2a16   Sfax Center 2024-06-24 06:10:00   \n",
       "1992  bf5f7820  dd49cdfe   Sfax Center 2025-01-06 21:20:00   \n",
       "1994  eadc0e36  cfcb50a1   Sfax_Center 2024-04-08 12:49:00   \n",
       "1995  c35f12c9  03d42852    Tunis Lake 2024-07-02 12:01:00   \n",
       "\n",
       "          check_out_time        duration  \n",
       "0    2025-06-08 18:40:00 0 days 00:38:00  \n",
       "1    2023-12-14 11:24:00 0 days 01:57:00  \n",
       "2    2023-03-29 11:16:00 0 days 01:13:00  \n",
       "3    2024-09-25 15:12:00 0 days 01:56:00  \n",
       "5    2025-06-14 09:50:00 0 days 01:45:00  \n",
       "...                  ...             ...  \n",
       "1990 2024-01-23 15:53:00 0 days 00:57:00  \n",
       "1991 2024-06-24 08:09:00 0 days 01:59:00  \n",
       "1992 2025-01-06 23:08:00 0 days 01:48:00  \n",
       "1994 2024-04-08 13:24:00 0 days 00:35:00  \n",
       "1995 2024-07-02 13:47:00 0 days 01:46:00  \n",
       "\n",
       "[1798 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Calculate duration for completed sessions only\n",
    "temp_df = visits_clean.dropna(subset=['check_out_time']).copy()\n",
    "temp_df['duration'] = temp_df['check_out_time'] - temp_df['check_in_time']\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0 days 01:10:30\n",
       "1      0 days 01:16:30\n",
       "2      0 days 01:16:30\n",
       "3      0 days 01:12:30\n",
       "5      0 days 01:14:30\n",
       "             ...      \n",
       "1990   0 days 01:17:00\n",
       "1991   0 days 01:15:00\n",
       "1992   0 days 01:15:00\n",
       "1994   0 days 01:14:30\n",
       "1995   0 days 01:10:30\n",
       "Name: duration, Length: 1798, dtype: timedelta64[ns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Get median duration per branch\n",
    "# transform() maps the median of the group back to every row based on its 'branch'\n",
    "branch_medians = temp_df.groupby('branch')['duration'].transform('median')\n",
    "branch_medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "branch\n",
       "Bizerte Port      0 days 01:11:30\n",
       "Nabeul            0 days 01:16:30\n",
       "Nabeul Plage      0 days 01:12:30\n",
       "Sfax Center       0 days 01:15:00\n",
       "Sfax_Center       0 days 01:14:30\n",
       "Sousse            0 days 01:15:00\n",
       "Sousse Corniche   0 days 01:14:00\n",
       "Tunis Lake        0 days 01:10:30\n",
       "tunis lake        0 days 01:17:00\n",
       "Name: duration, dtype: timedelta64[ns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Create a mapping of Branch -> Median Duration\n",
    "# This allows us to look up the duration for rows that are MISSING check_out_time\n",
    "medians_map = temp_df.groupby('branch')['duration'].median()\n",
    "medians_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visits dataset after cleaning: 1996 rows\n"
     ]
    }
   ],
   "source": [
    "# 3. Create a mapping of Branch -> Median Duration\n",
    "# This allows us to look up the duration for rows that are MISSING check_out_time\n",
    "medians_map = temp_df.groupby('branch')['duration'].median()\n",
    "global_median = temp_df['duration'].median()\n",
    "# 4. Apply the imputation\n",
    "def get_branch_duration(row):\n",
    "    # Try branch median, if branch has no data, use global median\n",
    "    return medians_map.get(row['branch'], global_median)\n",
    "mask = visits_clean['check_out_time'].isnull()\n",
    "# We apply the specific duration to each missing row\n",
    "visits_clean.loc[mask, 'check_out_time'] = visits_clean.loc[mask, 'check_in_time']+\\\n",
    "    visits_clean[mask].apply(get_branch_duration, axis=1)\n",
    "print(f\"Visits dataset after cleaning: {visits_clean.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(visits_clean['check_out_time'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Cleaning Payments Data\n",
    "- Remove negative amounts (assume refunds not needed for this BI dashboard)\n",
    "- Correct 'Data Entry' outliers (e.g., if price > 1000 for a month, it's likely a typo)\n",
    "- Normalize membership type naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 outliers. Correcting 10x entry errors...\n",
      "Payments dataset after cleaning: 1484 rows\n"
     ]
    }
   ],
   "source": [
    "payments_clean = payments_raw.copy()\n",
    "\n",
    "# 1. Remove negative amounts\n",
    "payments_clean = payments_clean[payments_clean['amount'] > 0]\n",
    "\n",
    "# 2. Handle 10x Entry Errors (Max expected price is roughly 1000 TND for Annual)\n",
    "outliers = payments_clean[payments_clean['amount'] > 1500]\n",
    "print(f\"Found {len(outliers)} outliers. Correcting 10x entry errors...\")\n",
    "payments_clean.loc[payments_clean['amount'] > 1500, 'amount'] = payments_clean['amount'] / 10\n",
    "\n",
    "# 3. Normalize Membership Types\n",
    "payments_clean['membership_type'] = payments_clean['membership_type'].str.capitalize()\n",
    "\n",
    "print(f\"Payments dataset after cleaning: {payments_clean.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative payments:  0\n",
      "number of null payments:  0\n",
      "number of outlier payments:  0\n",
      "unique membership types:  ['Monthly' 'Vip' 'Annual' 'Student' '6-month']\n"
     ]
    }
   ],
   "source": [
    "print(\"number of negative payments: \",payments_clean[payments_clean['amount'] < 0].shape[0])\n",
    "print(\"number of null payments: \",payments_clean[payments_clean['amount'].isnull()].shape[0])\n",
    "print(\"number of outlier payments: \", payments_clean[payments_clean['amount'] > 1500].shape[0])\n",
    "print(\"unique membership types: \",payments_clean['membership_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Loading (Saving the results)\n",
    "Now we save the clean datasets to `data/processed/` for downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL Complete! Clean datasets saved to data/processed/\n"
     ]
    }
   ],
   "source": [
    "members_clean.to_csv(os.path.join(PROCESSED_DATA_PATH, \"members_clean.csv\"), index=False)\n",
    "visits_clean.to_csv(os.path.join(PROCESSED_DATA_PATH, \"visits_clean.csv\"), index=False)\n",
    "payments_clean.to_csv(os.path.join(PROCESSED_DATA_PATH, \"payments_clean.csv\"), index=False)\n",
    "\n",
    "print(\"ETL Complete! Clean datasets saved to\", PROCESSED_DATA_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bi_py_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
